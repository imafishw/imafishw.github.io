[{"id":"3830d4aadcfd24ee5b2ca9f694f1c0fa","title":"RNN、LSTM原理以及代码实践1","content":"一、文本预处理以及词元化(基础)1、为什么要进行文本预处理以及词源化训练时用字符串直接进行训练是不好训练的，通常我们建立一个词表，例如基于词元级别(什么意思呢？其实就是拆分句子的过程以英语词为单位)于此同时,也有基于字符级的拆分来构建我们的词表 这里用的是李沐老师的d2l库中的数据\n12345678910111213141516171819import collectionsimport refrom d2l import torch as d2l #@saved2l.DATA_HUB[&#x27;time_machine&#x27;] = (d2l.DATA_URL + &#x27;timemachine.txt&#x27;,                                &#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;)def read_time_machine():  #@save    &quot;&quot;&quot;将时间机器数据集加载到文本行的列表中&quot;&quot;&quot;    with open(d2l.download(&#x27;time_machine&#x27;), &#x27;r&#x27;) as f:        lines = f.readlines()        # 十分暴力的预处理 一般标点符号还是保留的    return [re.sub(&#x27;[^A-Za-z]+&#x27;, &#x27; &#x27;, line).strip().lower() for line in lines]lines = read_time_machine()print(f&#x27;# 文本总行数: &#123;len(lines)&#125;&#x27;)print(lines[0])print(lines[10])\n\n\n# 文本总行数: 3221\nthe time machine by h g wells\ntwinkled and his usually pale face was flushed and animated the\n\n123456789101112def tokenize(lines, token=&#x27;word&#x27;):  #@save    &quot;&quot;&quot;将文本行拆分为单词或字符词元&quot;&quot;&quot;    if token == &#x27;word&#x27;:        return [line.split() for line in lines]    elif token == &#x27;char&#x27;:        return [list(line) for line in lines]    else:        print(&#x27;错误：未知词元类型：&#x27; + token)tokens = tokenize(lines)for i in range(11):    print(tokens[i])\n\n1234567891011[&#x27;the&#x27;, &#x27;time&#x27;, &#x27;machine&#x27;, &#x27;by&#x27;, &#x27;h&#x27;, &#x27;g&#x27;, &#x27;wells&#x27;][][][][][&#x27;i&#x27;][][][&#x27;the&#x27;, &#x27;time&#x27;, &#x27;traveller&#x27;, &#x27;for&#x27;, &#x27;so&#x27;, &#x27;it&#x27;, &#x27;will&#x27;, &#x27;be&#x27;, &#x27;convenient&#x27;, &#x27;to&#x27;, &#x27;speak&#x27;, &#x27;of&#x27;, &#x27;him&#x27;][&#x27;was&#x27;, &#x27;expounding&#x27;, &#x27;a&#x27;, &#x27;recondite&#x27;, &#x27;matter&#x27;, &#x27;to&#x27;, &#x27;us&#x27;, &#x27;his&#x27;, &#x27;grey&#x27;, &#x27;eyes&#x27;, &#x27;shone&#x27;, &#x27;and&#x27;][&#x27;twinkled&#x27;, &#x27;and&#x27;, &#x27;his&#x27;, &#x27;usually&#x27;, &#x27;pale&#x27;, &#x27;face&#x27;, &#x27;was&#x27;, &#x27;flushed&#x27;, &#x27;and&#x27;, &#x27;animated&#x27;, &#x27;the&#x27;]\n\n\n词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。 现在，让我们构建一个字典，通常也叫做词表（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为语料（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“”）； 序列开始词元（“”）； 序列结束词元（“”）。\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import collectionsclass Vocab:    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):        if tokens is None:            tokens = []        if reserved_tokens is None:            reserved_tokens = []        counter = count_corpus(tokens)        # 返回词出现的频率        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)        # 未知词元的索引为0        self.idx_to_token = [&#x27;&lt;unk&gt;&#x27;] + reserved_tokens        # [&#x27;&lt;unk&gt;&#x27;, &#x27;&lt;bos&gt;&#x27;, &#x27;&lt;eos&gt;&#x27;]。        # 变成这样的        # &#123;&#x27;&lt;unk&gt;&#x27;:0, &#x27;&lt;bos&gt;&#x27;:1, &#x27;&lt;eos&gt;&#x27;:2&#125;。        self.token_to_idx = &#123;token: idx                             for idx, token in enumerate(self.idx_to_token)&#125;        for token, freq in self._token_freqs:            # min_freq是我们规定的出现的最低频率次数 少于这个次数的词 不配进入词表            if freq &lt; min_freq:                break            if token not in self.token_to_idx:                # 构建索引以及对应的词元是什么                self.idx_to_token.append(token)                self.token_to_idx[token] = len(self.idx_to_token) - 1    def __len__(self):        return len(self.idx_to_token)    def __getitem__(self, tokens):        if not isinstance(tokens, (list, tuple)):            return self.token_to_idx.get(tokens, self.unk)        return [self.__getitem__(token) for token in tokens]    def to_tokens(self, indices):        if not isinstance(indices, (list, tuple)):            return self.idx_to_token[indices]        return [self.idx_to_token[index] for index in indices]    @property    def unk(self):  # 未知词元的索引为0        return 0    @property    def token_freqs(self):        return self._token_freqsdef count_corpus(tokens):  # @save    &quot;&quot;&quot;统计词元的频率&quot;&quot;&quot;    # 这里的tokens是1D列表或2D列表    if len(tokens) == 0 or isinstance(tokens[0], list):        # 将词元列表展平成一个列表        tokens = [token for line in tokens for token in line]    return collections.Counter(tokens)vocab = Vocab(tokens)print(list(vocab.token_to_idx.items())[:100])\n\n输出 按出现频率排序的一个列表\n\n[(‘‘, 0), (‘the’, 1), (‘i’, 2), (‘and’, 3), (‘of’, 4), (‘a’, 5), (‘to’, 6), (‘was’, 7), (‘in’, 8), (‘that’, 9), (‘my’, 10), (‘it’, 11), (‘had’, 12), (‘me’, 13), (‘as’, 14), (‘at’, 15), (‘for’, 16), (‘with’, 17), (‘but’, 18), (‘time’, 19), (‘were’, 20), (‘this’, 21), (‘you’, 22), (‘on’, 23), (‘then’, 24), (‘his’, 25), (‘there’, 26), (‘he’, 27), (‘have’, 28), (‘they’, 29), (‘from’, 30), (‘one’, 31), (‘all’, 32), (‘not’, 33), (‘into’, 34), (‘upon’, 35), (‘little’, 36), (‘so’, 37), (‘is’, 38), (‘came’, 39), (‘by’, 40), (‘some’, 41), (‘be’, 42), (‘no’, 43), (‘could’, 44), (‘their’, 45), (‘said’, 46), (‘saw’, 47), (‘down’, 48), (‘them’, 49), (‘machine’, 50), (‘which’, 51), (‘very’, 52), (‘or’, 53), (‘an’, 54), (‘we’, 55), (‘now’, 56), (‘what’, 57), (‘been’, 58), (‘these’, 59), (‘like’, 60), (‘her’, 61), (‘out’, 62), (‘seemed’, 63), (‘up’, 64), (‘man’, 65), (‘about’, 66), (‘s’, 67), (‘its’, 68), (‘thing’, 69), (‘again’, 70), (‘traveller’, 71), (‘would’, 72), (‘more’, 73), (‘white’, 74), (‘our’, 75), (‘thought’, 76), (‘felt’, 77), (‘when’, 78), (‘over’, 79), (‘weena’, 80), (‘still’, 81), (‘world’, 82), (‘myself’, 83), (‘even’, 84), (‘must’, 85), (‘through’, 86), (‘if’, 87), (‘hand’, 88), (‘went’, 89), (‘first’, 90), (‘are’, 91), (‘before’, 92), (‘last’, 93), (‘towards’, 94), (‘only’, 95), (‘people’, 96), (‘she’, 97), (‘morlocks’, 98), (‘see’, 99)]\n\n123for i in [0, 10]:    print(&#x27;文本:&#x27;, tokens[i])    print(&#x27;索引:&#x27;, vocab[tokens[i]])\n\n1234567891011121314def load_corpus_time_machine(max_tokens=-1):  #@save    &quot;&quot;&quot;返回时光机器数据集的词元索引列表和词表&quot;&quot;&quot;    lines = read_time_machine()    tokens = tokenize(lines, &#x27;char&#x27;)    vocab = Vocab(tokens)    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，    # 所以将所有文本行展平到一个列表中    corpus = [vocab[token] for line in tokens for token in line]    if max_tokens &gt; 0:        corpus = corpus[:max_tokens]    return corpus, vocabcorpus, vocab = load_corpus_time_machine()len(corpus), len(vocab)\n\n\n\n","slug":"first-page","date":"2025-02-11T12:41:20.000Z","categories_index":"","tags_index":"DeepLearn","author_index":"WillamNan"},{"id":"b9663f58f18133b35bfe243f3e916a80","title":"Hello World","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post1$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server1$ hexo server\n\nMore info: Server\nGenerate static files1$ hexo generate\n\nMore info: Generating\nDeploy to remote sites1$ hexo deploy\n\nMore info: Deployment\n","slug":"hello-world","date":"2025-02-11T12:38:59.673Z","categories_index":"","tags_index":"","author_index":"WillamNan"}]